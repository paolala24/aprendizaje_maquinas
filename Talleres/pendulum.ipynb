{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paolala24/aprendizaje_maquinas/blob/main/Talleres/pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d5647738",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-06-24T17:44:53.252585Z",
          "iopub.status.busy": "2025-06-24T17:44:53.252358Z",
          "iopub.status.idle": "2025-06-24T17:44:55.418373Z",
          "shell.execute_reply": "2025-06-24T17:44:55.417791Z"
        },
        "papermill": {
          "duration": 2.171912,
          "end_time": "2025-06-24T17:44:55.419822",
          "exception": false,
          "start_time": "2025-06-24T17:44:53.247910",
          "status": "completed"
        },
        "tags": [],
        "id": "d5647738"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bbef731",
      "metadata": {
        "papermill": {
          "duration": 0.002529,
          "end_time": "2025-06-24T17:44:55.425654",
          "exception": false,
          "start_time": "2025-06-24T17:44:55.423125",
          "status": "completed"
        },
        "tags": [],
        "id": "4bbef731"
      },
      "source": [
        "## 1.Instalación de dependencias:\n",
        "### Prompt 1:\n",
        "\n",
        "Quiero comenzar un ejercicio de aprendizaje por refuerzo continuo utilizando el entorno Pendulum-v1 de Gymnasium junto con la librería TF-Agents. Para ello, necesito que primero instales las dependencias necesarias, incluyendo tf-agents[reverb] y gymnasium[classic-control]. Luego, importa todas las librerías relevantes para trabajar con TF-Agents, TensorFlow, NumPy, Matplotlib y Gym. A continuación, carga el entorno Pendulum-v1 usando suite_gym de TF-Agents para obtener dos entornos: uno para entrenamiento y otro para evaluación, y conviértelos a objetos TFPyEnvironment. También quiero que imprimas la especificación de observaciones y acciones del entorno de entrenamiento para verificar que todo está funcionando correctamente. Además, crea un entorno adicional usando Gymnasium (con render_mode=\"rgb_array\") solo para visualización, y define una función llamada plot_environment() que renderice una imagen del entorno y la muestre usando Matplotlib. Por favor, organiza el código resultante en celdas, como si fuera a ejecutarse en un entorno tipo Jupyter Notebook o Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "545dd45a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:44:55.432324Z",
          "iopub.status.busy": "2025-06-24T17:44:55.431472Z",
          "iopub.status.idle": "2025-06-24T17:46:58.675188Z",
          "shell.execute_reply": "2025-06-24T17:46:58.674061Z"
        },
        "papermill": {
          "duration": 123.249425,
          "end_time": "2025-06-24T17:46:58.677796",
          "exception": false,
          "start_time": "2025-06-24T17:44:55.428371",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "545dd45a",
        "outputId": "620a5a8c-f1b3-4ed2-dd95-4e44ad55fa60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (3.1.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (11.2.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.17.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (4.25.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Requirement already satisfied: rlds in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: dm-reverb~=0.14.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (0.14.0)\n",
            "Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (2.15.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.9)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.11/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->dm-reverb~=0.14.0->tf-agents[reverb]) (25.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-agents[reverb] gymnasium[classic-control]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a03d25",
      "metadata": {
        "papermill": {
          "duration": 0.010016,
          "end_time": "2025-06-24T17:46:58.700187",
          "exception": false,
          "start_time": "2025-06-24T17:46:58.690171",
          "status": "completed"
        },
        "tags": [],
        "id": "32a03d25"
      },
      "source": [
        "## 2.Importación de Librerias y Creacion del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "47fdfeb0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:46:58.722588Z",
          "iopub.status.busy": "2025-06-24T17:46:58.722311Z",
          "iopub.status.idle": "2025-06-24T17:47:08.441641Z",
          "shell.execute_reply": "2025-06-24T17:47:08.440682Z"
        },
        "papermill": {
          "duration": 9.732444,
          "end_time": "2025-06-24T17:47:08.442852",
          "exception": false,
          "start_time": "2025-06-24T17:46:58.710408",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47fdfeb0",
        "outputId": "85fb3f73-bdf5-444e-ee2c-7dafee298f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Especificación de observaciones:\n",
            "BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32))\n",
            "\n",
            "Especificación de acciones:\n",
            "BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "# Nombre del entorno\n",
        "env_name = \"Pendulum-v1\"\n",
        "\n",
        "# Cargar entornos en modo Python (no-TensorFlow)\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "# Convertir a entornos compatibles con TensorFlow (TFPyEnvironment)\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "# Mostrar especificaciones de observación y acción\n",
        "print(\"Especificación de observaciones:\")\n",
        "print(train_env.observation_spec())\n",
        "\n",
        "print(\"\\nEspecificación de acciones:\")\n",
        "print(train_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a2ffcd",
      "metadata": {
        "papermill": {
          "duration": 0.010136,
          "end_time": "2025-06-24T17:47:08.463468",
          "exception": false,
          "start_time": "2025-06-24T17:47:08.453332",
          "status": "completed"
        },
        "tags": [],
        "id": "43a2ffcd"
      },
      "source": [
        "## 3.Entorno para visualización y función de renderizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "33f2b902",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:47:08.484604Z",
          "iopub.status.busy": "2025-06-24T17:47:08.484247Z",
          "iopub.status.idle": "2025-06-24T17:47:08.809350Z",
          "shell.execute_reply": "2025-06-24T17:47:08.808725Z"
        },
        "papermill": {
          "duration": 0.336901,
          "end_time": "2025-06-24T17:47:08.810398",
          "exception": false,
          "start_time": "2025-06-24T17:47:08.473497",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "33f2b902",
        "outputId": "39e5ce8e-5c7a-4ebf-c0dd-3653813b3f24"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Crear entorno para visualización con modo gráfico\n",
        "render_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "\n",
        "# Función de visualización con Matplotlib\n",
        "def plot_environment():\n",
        "    obs, _ = render_env.reset()\n",
        "    img = render_env.render()\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Visualización del entorno Pendulum-v1\")\n",
        "    plt.show()\n",
        "\n",
        "# Ejecutar visualización una vez\n",
        "plot_environment()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b09bf1",
      "metadata": {
        "papermill": {
          "duration": 0.010139,
          "end_time": "2025-06-24T17:47:08.831323",
          "exception": false,
          "start_time": "2025-06-24T17:47:08.821184",
          "status": "completed"
        },
        "tags": [],
        "id": "a7b09bf1"
      },
      "source": [
        "## 4.Definición de hiperparametros para DDPG\n",
        "\n",
        "### Prompt 2:\n",
        "Ahora necesito que definas los hiperparametros necesarios para entrenar un agente de DDPG en el entorno Pendulum-v1, usando TF-agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df254c74",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:47:08.852763Z",
          "iopub.status.busy": "2025-06-24T17:47:08.852534Z",
          "iopub.status.idle": "2025-06-24T17:47:08.857060Z",
          "shell.execute_reply": "2025-06-24T17:47:08.856544Z"
        },
        "papermill": {
          "duration": 0.016395,
          "end_time": "2025-06-24T17:47:08.858075",
          "exception": false,
          "start_time": "2025-06-24T17:47:08.841680",
          "status": "completed"
        },
        "tags": [],
        "id": "df254c74"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------\n",
        "# Parámetros globales de entrenamiento\n",
        "# -------------------------------------\n",
        "num_iterations = 100_000                # Número total de iteraciones de entrenamiento\n",
        "initial_collect_steps = 1000            # Pasos de recolección antes del entrenamiento\n",
        "collect_steps_per_iteration = 1         # Pasos de recolección por iteración\n",
        "replay_buffer_capacity = 100_000        # Capacidad del buffer de replay\n",
        "\n",
        "batch_size = 64                         # Tamaño del batch para entrenamiento\n",
        "critic_learning_rate = 1e-3             # Tasa de aprendizaje del crítico\n",
        "actor_learning_rate = 1e-4              # Tasa de aprendizaje del actor\n",
        "\n",
        "log_interval = 200                      # Frecuencia de impresión de logs\n",
        "eval_interval = 1000                    # Frecuencia de evaluación\n",
        "num_eval_episodes = 10                  # Número de episodios de evaluación\n",
        "\n",
        "# -------------------------------------\n",
        "# Parámetros de redes objetivo (target)\n",
        "# -------------------------------------\n",
        "tau = 0.005                             # Parámetro de actualización suave (soft update)\n",
        "gamma = 0.99                            # Factor de descuento (reward discount)\n",
        "\n",
        "# -------------------------------------\n",
        "# Arquitectura de redes\n",
        "# -------------------------------------\n",
        "actor_fc_layers = (400, 300)           # Capas ocultas del actor\n",
        "critic_obs_fc_layers = ()              # Capas para observaciones en el crítico (ninguna adicional)\n",
        "critic_action_fc_layers = ()           # Capas para acciones en el crítico (ninguna adicional)\n",
        "critic_joint_fc_layers = (400, 300)    # Capas conjuntas observación-acción en el crítico\n",
        "\n",
        "# -------------------------------------\n",
        "# Parámetros de ruido Ornstein-Uhlenbeck para exploración\n",
        "# -------------------------------------\n",
        "ou_stddev = 0.2                         # Desviación estándar del ruido OU\n",
        "ou_damping = 0.15                       # Término de amortiguamiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e255bfa",
      "metadata": {
        "papermill": {
          "duration": 0.009949,
          "end_time": "2025-06-24T17:47:08.878097",
          "exception": false,
          "start_time": "2025-06-24T17:47:08.868148",
          "status": "completed"
        },
        "tags": [],
        "id": "2e255bfa"
      },
      "source": [
        "## 5. Definición de las redes Actor y Crítica\n",
        "### Prompt 3:\n",
        "Sigue definiendo las redes neuronales necesarias para un agente DDPG (DdpgAgent) en el entorno Pendulum-v1 usando TF-Agents. Luego, define una red crítica que reciba como entrada una tupla (observación, acción). Usa los tamaños de capa definidos en los hiperparámetros. Al final, imprime una verificación que confirme que las redes fueron creadas correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2c2a9632",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:47:08.899531Z",
          "iopub.status.busy": "2025-06-24T17:47:08.899326Z",
          "iopub.status.idle": "2025-06-24T17:47:09.167516Z",
          "shell.execute_reply": "2025-06-24T17:47:09.166671Z"
        },
        "papermill": {
          "duration": 0.280428,
          "end_time": "2025-06-24T17:47:09.168681",
          "exception": false,
          "start_time": "2025-06-24T17:47:08.888253",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c2a9632",
        "outputId": "3997a8c4-de42-43ab-927f-8e008050a9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Red Actor creada: True\n",
            "✅ Red Crítico creada: True\n"
          ]
        }
      ],
      "source": [
        "from tf_agents.agents.ddpg.actor_network import ActorNetwork\n",
        "from tf_agents.agents.ddpg.critic_network import CriticNetwork\n",
        "from tf_agents.networks import utils\n",
        "from tf_agents.specs import tensor_spec\n",
        "\n",
        "# -------------------------------------\n",
        "# Especificaciones de entrada/salida\n",
        "# -------------------------------------\n",
        "observation_spec = train_env.observation_spec()\n",
        "action_spec = train_env.action_spec()\n",
        "time_step_spec = train_env.time_step_spec()\n",
        "\n",
        "# -------------------------------------\n",
        "# Red Actor: mapea observaciones → acciones\n",
        "# -------------------------------------\n",
        "actor_net = ActorNetwork(\n",
        "    input_tensor_spec=observation_spec,\n",
        "    output_tensor_spec=action_spec,\n",
        "    fc_layer_params=actor_fc_layers,\n",
        "    activation_fn=tf.keras.activations.relu,\n",
        "    name=\"ActorNetwork\"\n",
        ")\n",
        "\n",
        "# -------------------------------------\n",
        "# Red Crítica: mapea (observación, acción) → Q(s, a)\n",
        "# -------------------------------------\n",
        "critic_net = CriticNetwork(\n",
        "    input_tensor_spec=(observation_spec, action_spec),\n",
        "    observation_fc_layer_params=critic_obs_fc_layers,\n",
        "    action_fc_layer_params=critic_action_fc_layers,\n",
        "    joint_fc_layer_params=critic_joint_fc_layers,\n",
        "    activation_fn=tf.keras.activations.relu,\n",
        "    name=\"CriticNetwork\"\n",
        ")\n",
        "\n",
        "# -------------------------------------\n",
        "# Verificación\n",
        "# -------------------------------------\n",
        "print(\"✅ Red Actor creada:\", isinstance(actor_net, ActorNetwork))\n",
        "print(\"✅ Red Crítico creada:\", isinstance(critic_net, CriticNetwork))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b702b29",
      "metadata": {
        "papermill": {
          "duration": 0.012112,
          "end_time": "2025-06-24T17:47:09.191570",
          "exception": false,
          "start_time": "2025-06-24T17:47:09.179458",
          "status": "completed"
        },
        "tags": [],
        "id": "4b702b29"
      },
      "source": [
        "## 6. Creación del agente DDPG (DdpgAgent)\n",
        "### Prompt 4:\n",
        "Ahora quiero que sigas con la creación del agente DdopgAgent usando estas redes creadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cc0bb6b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:47:09.213487Z",
          "iopub.status.busy": "2025-06-24T17:47:09.212738Z",
          "iopub.status.idle": "2025-06-24T17:47:11.581028Z",
          "shell.execute_reply": "2025-06-24T17:47:11.580297Z"
        },
        "papermill": {
          "duration": 2.380569,
          "end_time": "2025-06-24T17:47:11.582296",
          "exception": false,
          "start_time": "2025-06-24T17:47:09.201727",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc0bb6b3",
        "outputId": "20f1fc56-8500-4837-c551-e9d9f22ed688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Agente DDPG creado y listo para entrenamiento.\n"
          ]
        }
      ],
      "source": [
        "from tf_agents.agents.ddpg.ddpg_agent import DdpgAgent\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.utils import common\n",
        "\n",
        "# -------------------------------------\n",
        "# Inicializadores (optimizers)\n",
        "# -------------------------------------\n",
        "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_learning_rate)\n",
        "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_learning_rate)\n",
        "\n",
        "# -------------------------------------\n",
        "# Agente DDPG\n",
        "# -------------------------------------\n",
        "ddpg_agent = DdpgAgent(\n",
        "    time_step_spec=train_env.time_step_spec(),\n",
        "    action_spec=action_spec,\n",
        "    actor_network=actor_net,\n",
        "    critic_network=critic_net,\n",
        "    actor_optimizer=actor_optimizer,\n",
        "    critic_optimizer=critic_optimizer,\n",
        "    ou_stddev=ou_stddev,\n",
        "    ou_damping=ou_damping,\n",
        "    target_update_tau=tau,\n",
        "    target_update_period=1,\n",
        "    gamma=gamma,\n",
        "    reward_scale_factor=1.0,\n",
        "    gradient_clipping=None,\n",
        "    train_step_counter=tf.Variable(0)\n",
        ")\n",
        "\n",
        "# Inicializar el agente (compilar redes y variables)\n",
        "ddpg_agent.initialize()\n",
        "\n",
        "# Confirmación\n",
        "print(\"✅ Agente DDPG creado y listo para entrenamiento.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd07edb0",
      "metadata": {
        "papermill": {
          "duration": 0.010267,
          "end_time": "2025-06-24T17:47:11.603504",
          "exception": false,
          "start_time": "2025-06-24T17:47:11.593237",
          "status": "completed"
        },
        "tags": [],
        "id": "dd07edb0"
      },
      "source": [
        "## 7. Replay Buffer y Política de Recolección"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "53633194",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:47:11.626170Z",
          "iopub.status.busy": "2025-06-24T17:47:11.625902Z",
          "iopub.status.idle": "2025-06-24T17:47:11.653325Z",
          "shell.execute_reply": "2025-06-24T17:47:11.652542Z"
        },
        "papermill": {
          "duration": 0.040098,
          "end_time": "2025-06-24T17:47:11.654492",
          "exception": false,
          "start_time": "2025-06-24T17:47:11.614394",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53633194",
        "outputId": "188be137-1c28-4f9e-c0f9-e0346e6ba67c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Replay Buffer inicializado con capacidad = 100000\n",
            "✅ Política de recolección (con OU) del agente configurada.\n"
          ]
        }
      ],
      "source": [
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "# -------------------------------------\n",
        "# Replay Buffer: para almacenar transiciones (s, a, r, s')\n",
        "# -------------------------------------\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=ddpg_agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_capacity\n",
        ")\n",
        "\n",
        "# -------------------------------------\n",
        "# Política de recolección (usa directamente la del agente DDPG con OU ya incluido)\n",
        "# -------------------------------------\n",
        "collect_policy = ddpg_agent.collect_policy\n",
        "\n",
        "# -------------------------------------\n",
        "# Confirmación\n",
        "# -------------------------------------\n",
        "print(\"✅ Replay Buffer inicializado con capacidad =\", replay_buffer_capacity)\n",
        "print(\"✅ Política de recolección (con OU) del agente configurada.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e7e349",
      "metadata": {
        "papermill": {
          "duration": 0.011086,
          "end_time": "2025-06-24T17:47:11.676181",
          "exception": false,
          "start_time": "2025-06-24T17:47:11.665095",
          "status": "completed"
        },
        "tags": [],
        "id": "d5e7e349"
      },
      "source": [
        "## 8. Driver para la recolección inicial de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c4157e56",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:47:11.698751Z",
          "iopub.status.busy": "2025-06-24T17:47:11.698523Z",
          "iopub.status.idle": "2025-06-24T17:47:22.878878Z",
          "shell.execute_reply": "2025-06-24T17:47:22.878065Z"
        },
        "papermill": {
          "duration": 11.192752,
          "end_time": "2025-06-24T17:47:22.880116",
          "exception": false,
          "start_time": "2025-06-24T17:47:11.687364",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4157e56",
        "outputId": "0427e454-2055-4c25-db79-fad2616b1a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Recolección inicial completada con 1000 pasos.\n"
          ]
        }
      ],
      "source": [
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.trajectories import trajectory\n",
        "\n",
        "# -------------------------------------\n",
        "# Métricas opcionales de recolección\n",
        "# -------------------------------------\n",
        "env_steps_metric = tf_metrics.EnvironmentSteps()\n",
        "avg_return_metric = tf_metrics.AverageReturnMetric()\n",
        "\n",
        "# -------------------------------------\n",
        "# Driver para recolección de pasos (experiencias iniciales)\n",
        "# -------------------------------------\n",
        "initial_collect_driver = DynamicStepDriver(\n",
        "    env=train_env,\n",
        "    policy=collect_policy,\n",
        "    observers=[replay_buffer.add_batch, env_steps_metric],\n",
        "    num_steps=initial_collect_steps\n",
        ")\n",
        "\n",
        "# Ejecutar recolección inicial\n",
        "initial_collect_driver.run()\n",
        "\n",
        "print(f\"✅ Recolección inicial completada con {initial_collect_steps} pasos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b51b3a2c",
      "metadata": {
        "papermill": {
          "duration": 0.010516,
          "end_time": "2025-06-24T17:47:22.946180",
          "exception": false,
          "start_time": "2025-06-24T17:47:22.935664",
          "status": "completed"
        },
        "tags": [],
        "id": "b51b3a2c"
      },
      "source": [
        "## 9. Entrenamiento del agente DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5f621489",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-24T17:47:22.968471Z",
          "iopub.status.busy": "2025-06-24T17:47:22.967928Z",
          "iopub.status.idle": "2025-06-24T18:36:37.558158Z",
          "shell.execute_reply": "2025-06-24T18:36:37.557296Z"
        },
        "papermill": {
          "duration": 2954.60247,
          "end_time": "2025-06-24T18:36:37.559276",
          "exception": false,
          "start_time": "2025-06-24T17:47:22.956806",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f621489",
        "outputId": "32980f51-3dbf-4b90-9660-0613d75a651f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Entrenando agente DDPG...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 5/100000 [00:23<95:57:05,  3.45s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 0: Recompensa promedio = -1243.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1005/100000 [01:12<34:54:58,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 1000: Recompensa promedio = -1485.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 2005/100000 [02:01<27:49:52,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 2000: Recompensa promedio = -1146.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3006/100000 [02:48<28:22:15,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 3000: Recompensa promedio = -750.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 4006/100000 [03:36<33:24:27,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 4000: Recompensa promedio = -1008.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 5006/100000 [04:25<25:18:08,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 5000: Recompensa promedio = -176.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 6006/100000 [05:12<24:16:30,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 6000: Recompensa promedio = -151.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 7004/100000 [06:00<34:33:01,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 7000: Recompensa promedio = -152.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 8005/100000 [06:47<28:22:33,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 8000: Recompensa promedio = -133.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 9006/100000 [07:34<22:50:06,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 9000: Recompensa promedio = -131.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 10006/100000 [08:22<26:42:26,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 10000: Recompensa promedio = -190.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 11005/100000 [09:09<24:45:11,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 11000: Recompensa promedio = -138.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 12005/100000 [09:58<30:57:08,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 12000: Recompensa promedio = -158.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 13006/100000 [10:47<33:44:55,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 13000: Recompensa promedio = -145.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 14006/100000 [11:34<26:41:47,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 14000: Recompensa promedio = -147.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 15006/100000 [12:22<21:58:33,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 15000: Recompensa promedio = -74.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 16006/100000 [13:08<24:03:13,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 16000: Recompensa promedio = -162.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 17005/100000 [13:56<26:18:32,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 17000: Recompensa promedio = -170.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 18005/100000 [14:43<25:00:59,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 18000: Recompensa promedio = -182.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 19005/100000 [15:32<23:59:30,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 19000: Recompensa promedio = -124.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 20005/100000 [16:27<28:26:06,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 20000: Recompensa promedio = -156.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 21005/100000 [17:15<20:53:53,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 21000: Recompensa promedio = -121.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 22006/100000 [18:01<22:40:34,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 22000: Recompensa promedio = -128.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 23006/100000 [18:49<22:38:14,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 23000: Recompensa promedio = -165.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 24006/100000 [19:35<21:05:49,  1.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 24000: Recompensa promedio = -48.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 25006/100000 [20:21<19:41:02,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 25000: Recompensa promedio = -95.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 26005/100000 [21:07<19:59:22,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 26000: Recompensa promedio = -163.04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 27004/100000 [21:53<24:02:09,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 27000: Recompensa promedio = -120.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 28006/100000 [22:38<22:40:50,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 28000: Recompensa promedio = -167.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 29006/100000 [23:23<17:01:58,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 29000: Recompensa promedio = -98.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 30006/100000 [24:08<19:14:30,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 30000: Recompensa promedio = -145.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 31006/100000 [24:54<18:43:56,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 31000: Recompensa promedio = -133.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 32004/100000 [25:39<21:03:54,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 32000: Recompensa promedio = -131.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 33006/100000 [26:27<27:31:08,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 33000: Recompensa promedio = -158.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 34006/100000 [27:13<17:06:37,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 34000: Recompensa promedio = -158.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 35006/100000 [27:58<15:30:30,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 35000: Recompensa promedio = -178.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 36005/100000 [28:44<17:02:54,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 36000: Recompensa promedio = -170.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 37005/100000 [29:29<16:18:49,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 37000: Recompensa promedio = -135.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 38006/100000 [30:14<22:04:34,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 38000: Recompensa promedio = -154.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 39006/100000 [31:00<17:34:12,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 39000: Recompensa promedio = -179.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 40006/100000 [31:45<14:35:23,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 40000: Recompensa promedio = -182.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 41006/100000 [32:31<16:50:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 41000: Recompensa promedio = -168.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 42004/100000 [33:17<19:13:51,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 42000: Recompensa promedio = -134.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 43006/100000 [34:03<18:47:15,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 43000: Recompensa promedio = -157.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 44006/100000 [34:50<16:34:07,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 44000: Recompensa promedio = -185.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 45004/100000 [35:37<15:21:26,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 45000: Recompensa promedio = -169.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 46006/100000 [36:24<13:51:19,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 46000: Recompensa promedio = -155.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 47006/100000 [37:12<13:44:35,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 47000: Recompensa promedio = -102.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 48005/100000 [37:59<13:59:25,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 48000: Recompensa promedio = -167.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 49004/100000 [38:46<15:57:01,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 49000: Recompensa promedio = -132.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 50006/100000 [39:32<16:57:42,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 50000: Recompensa promedio = -110.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 51005/100000 [40:21<17:19:37,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 51000: Recompensa promedio = -154.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 52006/100000 [41:08<12:01:01,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 52000: Recompensa promedio = -130.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 53006/100000 [41:55<12:33:47,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 53000: Recompensa promedio = -98.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 54005/100000 [42:40<14:06:46,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 54000: Recompensa promedio = -143.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 55004/100000 [43:25<11:47:04,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 55000: Recompensa promedio = -129.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 56006/100000 [44:10<13:59:00,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 56000: Recompensa promedio = -165.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 57005/100000 [44:57<11:25:39,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 57000: Recompensa promedio = -146.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 58006/100000 [45:43<10:36:34,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 58000: Recompensa promedio = -130.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 59006/100000 [46:30<13:06:26,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 59000: Recompensa promedio = -159.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 60006/100000 [47:17<10:47:41,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 60000: Recompensa promedio = -149.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 61004/100000 [48:04<12:30:21,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 61000: Recompensa promedio = -164.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 62008/100000 [48:50<12:07:01,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 62000: Recompensa promedio = -109.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 63006/100000 [49:36<9:42:54,  1.06it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 63000: Recompensa promedio = -117.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 64006/100000 [50:21<10:07:11,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 64000: Recompensa promedio = -130.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 65006/100000 [51:08<11:14:28,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 65000: Recompensa promedio = -169.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 66006/100000 [51:55<8:39:25,  1.09it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 66000: Recompensa promedio = -150.08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 67007/100000 [52:40<7:35:59,  1.21it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 67000: Recompensa promedio = -133.09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 68006/100000 [53:26<7:52:55,  1.13it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 68000: Recompensa promedio = -110.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▉   | 69005/100000 [54:13<8:29:21,  1.01it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 69000: Recompensa promedio = -121.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 70005/100000 [55:00<8:30:52,  1.02s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 70000: Recompensa promedio = -182.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 71005/100000 [55:48<11:26:10,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 71000: Recompensa promedio = -128.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 72006/100000 [56:36<7:51:55,  1.01s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 72000: Recompensa promedio = -129.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 73006/100000 [57:24<9:22:40,  1.25s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 73000: Recompensa promedio = -139.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 74004/100000 [58:11<7:59:06,  1.11s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 74000: Recompensa promedio = -146.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 75006/100000 [58:59<8:17:09,  1.19s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 75000: Recompensa promedio = -116.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 76006/100000 [59:47<6:44:51,  1.01s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 76000: Recompensa promedio = -116.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 77006/100000 [1:00:34<5:44:18,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 77000: Recompensa promedio = -96.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 78006/100000 [1:01:21<5:15:24,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 78000: Recompensa promedio = -171.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|███████▉  | 79005/100000 [1:02:08<8:40:05,  1.49s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 79000: Recompensa promedio = -133.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 80006/100000 [1:02:55<5:09:36,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 80000: Recompensa promedio = -128.63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 81005/100000 [1:03:42<5:20:26,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 81000: Recompensa promedio = -548.27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 82006/100000 [1:04:28<6:39:19,  1.33s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 82000: Recompensa promedio = -104.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 83006/100000 [1:05:15<4:33:39,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 83000: Recompensa promedio = -155.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 84004/100000 [1:06:02<4:43:28,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 84000: Recompensa promedio = -100.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 85005/100000 [1:06:50<5:01:19,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 85000: Recompensa promedio = -231.09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 86006/100000 [1:07:38<4:57:14,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 86000: Recompensa promedio = -177.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 87006/100000 [1:08:26<3:51:32,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 87000: Recompensa promedio = -173.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 88%|████████▊ | 88006/100000 [1:09:14<3:09:54,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 88000: Recompensa promedio = -158.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 89005/100000 [1:10:00<3:10:51,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 89000: Recompensa promedio = -110.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 90005/100000 [1:10:47<2:59:21,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 90000: Recompensa promedio = -134.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 91006/100000 [1:11:34<2:25:01,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 91000: Recompensa promedio = -145.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 92004/100000 [1:12:21<2:32:50,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 92000: Recompensa promedio = -90.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 93006/100000 [1:13:09<2:37:59,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 93000: Recompensa promedio = -181.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 94006/100000 [1:13:56<1:47:15,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 94000: Recompensa promedio = -132.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 95004/100000 [1:14:43<1:41:34,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 95000: Recompensa promedio = -196.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 96005/100000 [1:15:28<1:18:43,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 96000: Recompensa promedio = -105.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 97006/100000 [1:16:14<42:53,  1.16it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 97000: Recompensa promedio = -130.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 98005/100000 [1:17:00<32:59,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 98000: Recompensa promedio = -395.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 99006/100000 [1:17:46<19:23,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteración 99000: Recompensa promedio = -159.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [1:18:12<00:00, 21.31it/s]\n"
          ]
        }
      ],
      "source": [
        "from tf_agents.utils.common import function\n",
        "from tqdm import trange\n",
        "\n",
        "# -------------------------------------\n",
        "# Dataset del buffer para muestreo\n",
        "# -------------------------------------\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "\n",
        "# -------------------------------------\n",
        "# Entrenamiento en modo tf.function\n",
        "# -------------------------------------\n",
        "ddpg_agent.train = function(ddpg_agent.train)\n",
        "\n",
        "# -------------------------------------\n",
        "# Métricas para seguimiento\n",
        "# -------------------------------------\n",
        "train_rewards = []\n",
        "\n",
        "print(\"🚀 Entrenando agente DDPG...\\n\")\n",
        "\n",
        "for step in trange(num_iterations):\n",
        "    # Recolectar nuevos pasos\n",
        "    time_step = train_env.current_time_step()\n",
        "    action = collect_policy.action(time_step)\n",
        "    next_time_step = train_env.step(action.action)\n",
        "    traj = trajectory.from_transition(time_step, action, next_time_step)\n",
        "    replay_buffer.add_batch(traj)\n",
        "\n",
        "    # Entrenar con muestra del buffer\n",
        "    experience, _ = next(iterator)\n",
        "    train_loss = ddpg_agent.train(experience).loss\n",
        "\n",
        "    # Evaluación periódica\n",
        "    if step % eval_interval == 0:\n",
        "        total_return = 0.0\n",
        "        for _ in range(num_eval_episodes):\n",
        "            time_step = eval_env.reset()\n",
        "            episode_return = 0.0\n",
        "            while not time_step.is_last():\n",
        "                action = ddpg_agent.policy.action(time_step)\n",
        "                time_step = eval_env.step(action.action)\n",
        "                episode_return += time_step.reward.numpy()[0]\n",
        "            total_return += episode_return\n",
        "        avg_return = total_return / num_eval_episodes\n",
        "        train_rewards.append(avg_return)\n",
        "        print(f\"Iteración {step}: Recompensa promedio = {avg_return:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31040,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3114.96464,
      "end_time": "2025-06-24T18:36:41.727501",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-24T17:44:46.762861",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}